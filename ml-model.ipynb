{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Attack Risk Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import tensorflow as tf\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/heart_attack_dataset.csv')\n",
    "\n",
    "if 'Patient ID' in df.columns:\n",
    "    df.drop(columns=['Patient ID', 'Sex','Blood Pressure', 'Sedentary Hours Per Day', 'BMI', 'Country', 'Continent', 'Hemisphere', 'Exercise Hours Per Week', 'Diet'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_dataset/heart_attack_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"hanhanhanny-pipeline\"\n",
    "SCHEMA_PIPELINE_NAME = \"heart-attack-schema\"\n",
    "\n",
    "# Directory to store pipeline artifacts\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "\n",
    "# Path to a SQLite DB file for MLMD storage\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "# Directory to export created models\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Interactive Context\n",
    "DATA_ROOT = \"clean_dataset\"  # the data is placed in the \"clean_dataset\" directory\n",
    "interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = example_gen_pb2.Output(\n",
    "    split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
    "    ])\n",
    ")\n",
    "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "    examples = example_gen.outputs[\"examples\"]\n",
    ")\n",
    "interactive_context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(statistics_gen.outputs[\"statistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(    statistics=statistics_gen.outputs[\"statistics\"]\n",
    ")\n",
    "interactive_context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(schema_gen.outputs[\"schema\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs[\"statistics\"],\n",
    "    schema=schema_gen.outputs[\"schema\"]\n",
    ")\n",
    "\n",
    "interactive_context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(example_validator.outputs[\"anomalies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE_FILE = \"heart_attack_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import os\n",
    "\n",
    "# Set a custom temporary directory\n",
    "os.environ['TF_TFT_TMP_DIR'] = '/path/to/your/temp/dir'\n",
    "\n",
    "def transformed_key(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "    \n",
    "    Args:\n",
    "        inputs: map from feature keys to raw features.\n",
    "    \n",
    "    Return:\n",
    "        outputs: map from feature keys to transformed features.    \n",
    "\n",
    "    Description:\n",
    "        - apply one hot encoding to categorical features\n",
    "        - apply standardization to float features and int features that are not binary\n",
    "        - apply renaming of transformed features except for one hot encoded features\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = {}\n",
    "\n",
    "    # Standardize numerical features\n",
    "    outputs[transformed_key(\"Age\")] = tf.cast(inputs[\"Age\"], tf.int64)\n",
    "    outputs[transformed_key(\"Cholesterol\")] = tft.scale_to_0_1(inputs[\"Cholesterol\"])\n",
    "    outputs[transformed_key(\"Triglycerides\")] = tft.scale_to_0_1(inputs[\"Triglycerides\"])\n",
    "    outputs[transformed_key(\"Income\")] = tft.scale_to_0_1(inputs[\"Income\"])\n",
    "    outputs[transformed_key(\"Heart_Rate\")] = tft.scale_to_0_1(inputs[\"Heart Rate\"])\n",
    "    outputs[transformed_key(\"Stress_Level\")] = tft.scale_to_0_1(inputs[\"Stress Level\"])\n",
    "    outputs[transformed_key(\"Physical_Activity_Days_Per_Week\")] = tft.scale_to_0_1(inputs[\"Physical Activity Days Per Week\"])\n",
    "    outputs[transformed_key(\"Sleep_Hours_Per_Day\")] = tft.scale_to_0_1(inputs[\"Sleep Hours Per Day\"])\n",
    "\n",
    "    # Binary features (no transformation)\n",
    "    outputs[\"Smoking\"] = inputs[\"Smoking\"]\n",
    "    outputs[\"Diabetes\"] = inputs[\"Diabetes\"]\n",
    "    outputs[\"Family_History\"] = inputs[\"Family History\"]\n",
    "    outputs[\"Obesity\"] = inputs[\"Obesity\"]\n",
    "    outputs[\"Alcohol_Consumption\"] = inputs[\"Alcohol Consumption\"]\n",
    "    outputs[\"Previous_Heart_Problems\"] = inputs[\"Previous Heart Problems\"]\n",
    "    outputs[\"Medication_Use\"] = inputs[\"Medication Use\"]\n",
    "\n",
    "    # Target feature\n",
    "    outputs[\"Heart_Attack_Risk\"] = tf.cast(inputs[\"Heart Attack Risk\"], tf.int64)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs[\"examples\"],\n",
    "    schema=schema_gen.outputs[\"schema\"],\n",
    "    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)\n",
    ")\n",
    "interactive_context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_TUNER_MODULE_FILE = \"heart_attack_trainer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_TUNER_MODULE_FILE}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft \n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from keras_tuner.engine import base_tuner\n",
    "from keras_tuner import RandomSearch, HyperParameters\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from typing import NamedTuple, Dict, Text, Any\n",
    "\n",
    "LABEL_KEY = \"Heart Attack Risk\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "def get_hyperparameters() -> HyperParameters:\n",
    "    \"\"\"Returns hyperparameters for building model\"\"\"\n",
    "    hp = HyperParameters()\n",
    "    hp.Int('units', min_value=32, max_value=512, step=32, default=128)\n",
    "    hp.Int('num_layers', min_value=1, max_value=4, step=1, default=3)\n",
    "    hp.Float('learning_rate', min_value=1e-2, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "    return hp\n",
    "\n",
    "def input_fn(file_pattern, \n",
    "             tf_transform_output,\n",
    "             num_epochs=None,\n",
    "             batch_size=64)->tf.data.Dataset:\n",
    "    \"\"\"Get post_transform feature & create batches of data\"\"\"\n",
    "    \n",
    "    # Get post_transform feature spec\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "    \n",
    "    # create batches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=LABEL_KEY)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    return dataset\n",
    "\n",
    "def model_builder(hparams: HyperParameters):\n",
    "    \"\"\"Build machine learning model\"\"\"\n",
    "    inputs = {\n",
    "        transformed_name('Age'): tf.keras.Input(shape=(1,), name=transformed_name('Age'), dtype=tf.int64),\n",
    "        transformed_name('Cholesterol'): tf.keras.Input(shape=(1,), name=transformed_name('Cholesterol'), dtype=tf.int64),\n",
    "        transformed_name('Triglycerides'): tf.keras.Input(shape=(1,), name=transformed_name('Triglycerides'), dtype=tf.int64),\n",
    "        transformed_name('Income'): tf.keras.Input(shape=(1,), name=transformed_name('Income'), dtype=tf.int64),\n",
    "        transformed_name('Heart_Rate'): tf.keras.Input(shape=(1,), name=transformed_name('Heart_Rate'), dtype=tf.int64),\n",
    "        transformed_name('Stress_Level'): tf.keras.Input(shape=(1,), name=transformed_name('Stress_Level'), dtype=tf.int64),\n",
    "        transformed_name('Physical_Activity_Days_Per_Week'): tf.keras.Input(shape=(1,), name=transformed_name('Physical_Activity_Days_Per_Week'), dtype=tf.int64),\n",
    "        transformed_name('Sleep_Hours_Per_Day'): tf.keras.Input(shape=(1,), name=transformed_name('Sleep_Hours_Per_Day'), dtype=tf.int64),\n",
    "        'Smoking': tf.keras.Input(shape=(1,), name='Smoking', dtype=tf.int64),\n",
    "        'Diabetes': tf.keras.Input(shape=(1,), name='Diabetes', dtype=tf.int64),\n",
    "        'Family_History': tf.keras.Input(shape=(1,), name='Family_History', dtype=tf.int64),\n",
    "        'Obesity': tf.keras.Input(shape=(1,), name='Obesity', dtype=tf.int64),\n",
    "        'Alcohol_Consumption': tf.keras.Input(shape=(1,), name='Alcohol_Consumption', dtype=tf.int64),\n",
    "        'Previous_Heart_Problems': tf.keras.Input(shape=(1,), name='Previous_Heart_Problems', dtype=tf.int64),\n",
    "        'Medication_Use': tf.keras.Input(shape=(1,), name='Medication_Use', dtype=tf.int64)\n",
    "    }\n",
    "    \n",
    "    # Combine all inputs into a single tensor\n",
    "    concatenated_inputs = layers.Concatenate()(list(inputs.values()))\n",
    "\n",
    "    x = layers.Dense(hparams.get('units'), activation='relu')(concatenated_inputs)\n",
    "    for _ in range(hparams.get('num_layers') - 1):\n",
    "        x = layers.Dense(hparams.get('units') // 2, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hparams.get('learning_rate')),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Tuner component will run this function\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', RandomSearch),\n",
    "                                             ('fit_kwargs', Dict[Text, Any])])\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    \"\"\"\n",
    "    Build the tuner using the KerasTuner API.\n",
    "    Args:\n",
    "        fn_args: Holds args as name/value pairs.\n",
    "        - working_dir: working dir for tuning.\n",
    "        - train_files: List of file paths containing training tf.Example data.\n",
    "        - eval_files: List of file paths containing eval tf.Example data.\n",
    "        - train_steps: number of train steps.\n",
    "        - eval_steps: number of eval steps.\n",
    "        - schema_path: optional schema of the input data.\n",
    "        - transform_graph_path: optional transform graph produced by TFT.\n",
    "    Returns:\n",
    "        A namedtuple contains the following:\n",
    "        - tuner: A RandomSearch tuner that will be used for tuning.\n",
    "        - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
    "                        model, e.g., the training and validation dataset. Required\n",
    "                        args depend on the above tuner's implementation.\n",
    "    \"\"\"\n",
    "    hp = get_hyperparameters()\n",
    "    # Define tuner\n",
    "    tuner = RandomSearch(\n",
    "        model_builder,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_trials=30,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='heart_attack_risk_classification',\n",
    "        hyperparameters=hp\n",
    "    )\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            'x': train_set,\n",
    "            'validation_data': eval_set\n",
    "        }\n",
    "    )\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "        \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, update_freq='batch'\n",
    "    )\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    # Load the transform output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    \n",
    "    # Create batches of data\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)\n",
    "\n",
    "    hp = get_hyperparameters()\n",
    "    model = model_builder(hp)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        validation_data=eval_set,\n",
    "        callbacks=[tensorboard_callback, es, mc],\n",
    "        steps_per_epoch=100, \n",
    "        validation_steps=100,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    signatures = {\n",
    "        'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "            tf.TensorSpec(\n",
    "                shape=[None],\n",
    "                dtype=tf.string,\n",
    "                name='examples'))\n",
    "    }\n",
    "    \n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Tuner\n",
    "from tfx.proto import trainer_pb2\n",
    " \n",
    "tuner = Tuner(\n",
    "    module_file=os.path.abspath(TRAINER_TUNER_MODULE_FILE),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=500),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=100)\n",
    "    )\n",
    " \n",
    "interactive_context.run(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import trainer_pb2\n",
    " \n",
    "trainer = Trainer(\n",
    "    module_file = os.path.abspath(TRAINER_TUNER_MODULE_FILE),\n",
    "    examples = transform.outputs['transformed_examples'],\n",
    "    transform_graph = transform.outputs['transform_graph'],\n",
    "    schema = schema_gen.outputs['schema'],\n",
    "    hyperparameters = tuner.outputs['best_hyperparameters'],\n",
    "    train_args = trainer_pb2.TrainArgs(splits=['train']),\n",
    "    eval_args = trainer_pb2.EvalArgs(splits=['eval'])\n",
    ")\n",
    "\n",
    "interactive_context.run(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
